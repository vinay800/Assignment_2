{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Language Modeling and Smoothing\n",
    "1. Download any of these text books from Project Gutenberg\n",
    "a. Alice in Wonderland: ​ ​Alice’s Adventures in Wonderland\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "url=\"http://www.gutenberg.org/files/11/11-0.txt\" # url for downloading \"alice_in_wonderland.txt\"\n",
    "response = request.urlopen(url)\n",
    "whole_text = response.read().decode('utf8')\n",
    "# Now Compute Tokens\n",
    "#from nltk import word_tokenize\n",
    "#tokens = word_tokenize(whole_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- Parse the dataset into sentences using sentence ​tokenizer ​ and divide it into 80/20 ratio. Keep\n",
    "80% dataset for training N-grams and keep 20% for test. You can filter out unnecessary symbols,\n",
    "newlines, etc. You can add symbols <s> and </s> to mark sentence start and end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Total Sentences: 1093\n",
      "Length of train_samples sentences: 874\n",
      "Length of test_samples sentences: 219\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenized_list = sent_tokenize(whole_text)\n",
    "print('Length of Total Sentences:',len(sent_tokenized_list))\n",
    "import random\n",
    "import math\n",
    "\n",
    "random.shuffle(sent_tokenized_list) # randomly suffle the list\n",
    "train_percentage=80\n",
    "test_percentage=20\n",
    "\n",
    "no_of_training_sample=math.floor((train_percentage*len(sent_tokenized_list))/100)\n",
    "train_samples=sent_tokenized_list[:no_of_training_sample] # This variable store the 80% of the training sentences\n",
    "test_samples=sent_tokenized_list[no_of_training_sample:]# This variable store the 20% of the training sentences\n",
    "print('Length of train_samples sentences:',len(train_samples))\n",
    "print('Length of test_samples sentences:',len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['However, she soon\\r\\nmade out that she was in the pool of tears which she had wept when she\\r\\nwas nine feet high.',\n",
       " 'I’ve often seen a cat without a grin,’ thought Alice; ‘but a grin\\r\\nwithout a cat!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[2:4] # print some sample train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add start and end symbols to the train and test samples (same thing can be done on 'sent_tokenized_list')\n",
    "for num in range(len(train_samples)):\n",
    "    train_samples[num]='<s> '+train_samples[num]+' </s>'\n",
    "    \n",
    "for num in range(len(test_samples)):\n",
    "    test_samples[num]='<s> '+test_samples[num]+' </s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> However, she soon\\r\\nmade out that she was in the pool of tears which she had wept when she\\r\\nwas nine feet high. </s>',\n",
       " '<s> I’ve often seen a cat without a grin,’ thought Alice; ‘but a grin\\r\\nwithout a cat! </s>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_samples[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-Compute MLE for unigram, bigram, trigrams and quadgrams. How many n-grams are possible\n",
    "and how many actually exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 29465\n",
      "Type: 6019\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "# Now Compute Tokens for checking purpose\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "tokens_tmp =whole_text.split()\n",
    "\n",
    "print('Tokens:',len(tokens_tmp))\n",
    "\n",
    "# Now compute type\n",
    "import numpy as np\n",
    "type_tmp=np.unique(tokens_tmp)\n",
    "print(\"Type:\",len(type_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible unigrams =6019\n",
      "Total unigrams =6019\n"
     ]
    }
   ],
   "source": [
    "# MLE for unigram \n",
    "from nltk.probability import FreqDist\n",
    "output = list(ngrams(whole_text.split(), 1))\n",
    "fdist1 = FreqDist(output)\n",
    "# Uncomment the following two lines to print the probablities\n",
    "#for item in fdist1.keys():\n",
    "#    print('Prob of', item, 'is', fdist1[item]/len(fdist1))\n",
    "\n",
    "# mle_unigram is a dictionary, which stores n-grams and corresponding probablities\n",
    "mle_unigram={}\n",
    "for key, value in fdist1.items():\n",
    "    mle_unigram[key]=fdist1[key]/len(output)\n",
    "    \n",
    "print(\"Possible unigrams ={}\".format(len(type_tmp))) #type_tmp is vocabulary size\n",
    "print(\"Total unigrams ={}\".format(len(mle_unigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible bigrams =36228361\n",
      "Total bigrams =19419\n"
     ]
    }
   ],
   "source": [
    "# MLE for bigram \n",
    "output = list(ngrams(whole_text.split(), 2))\n",
    "fdist1 = FreqDist(output)\n",
    "# Uncomment the following two lines to print the probablities\n",
    "#for item in fdist1.keys():\n",
    "#    print('Prob of', item, 'is', fdist1[item]/len(fdist1))\n",
    "\n",
    "# mle_bigram is a dictionary, which stores n-grams and corresponding probablities\n",
    "mle_bigram={}\n",
    "for key, value in fdist1.items():\n",
    "    mle_bigram[key]=fdist1[key]/len(output)\n",
    "    \n",
    "print(\"Possible bigrams ={}\".format(len(type_tmp)**2))\n",
    "print(\"Total bigrams ={}\".format(len(mle_bigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible trigrams =218058504859\n",
      "Total trigrams =26595\n"
     ]
    }
   ],
   "source": [
    "# MLE for trigram \n",
    "output = list(ngrams(whole_text.split(), 3))\n",
    "fdist1 = FreqDist(output)\n",
    "# Uncomment the following two lines to print the probablities\n",
    "#for item in fdist1.keys():\n",
    "#    print('Prob of', item, 'is', fdist1[item]/len(fdist1))\n",
    "    \n",
    "# mle_trigram is a dictionary, which stores n-grams and corresponding probablities    \n",
    "mle_trigram={}\n",
    "for key, value in fdist1.items():\n",
    "    mle_trigram[key]=fdist1[key]/len(output)\n",
    "    \n",
    "    \n",
    "print(\"Possible trigrams ={}\".format(len(type_tmp)**3))\n",
    "print(\"Total trigrams ={}\".format(len(mle_trigram)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible quadgrams =1312494140746321\n",
      "Total quadgrams =28552\n"
     ]
    }
   ],
   "source": [
    "# MLE for quadgram \n",
    "output = list(ngrams(whole_text.split(), 4))\n",
    "fdist1 = FreqDist(output)\n",
    "# Uncomment the following two lines to print the probablities\n",
    "#for item in fdist1.keys():\n",
    "#    print('Prob of', item, 'is', fdist1[item]/len(fdist1))\n",
    "\n",
    "# mle_quadgram is a dictionary, which stores n-grams and corresponding probablities    \n",
    "mle_quadgram={}\n",
    "for key, value in fdist1.items():\n",
    "    mle_quadgram[key]=fdist1[key]/len(fdist1)\n",
    "\n",
    "    \n",
    "print(\"Possible quadgrams ={}\".format(len(type_tmp)**4))\n",
    "print(\"Total quadgrams ={}\".format(len(mle_quadgram)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Develop a system that has two functions:\n",
    "\n",
    "a. Generator(model_name): generates sentences by utilizing MLEs from specified\n",
    "n-gram model. Sampling from multinomial distribution can be done using a\n",
    "predefined ​function ​. Note, 5-10 sentences would suffice for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts integer values as the input argument(for example model_name=1 corresponds to unigram)\n",
    "# model_name=n, in general generates sentences using n-gram model(n=2,3,4....)\n",
    "# And generates sentences by utilizing MLEs from specified n-gram model using the corpus whole_text\n",
    "\n",
    "# Size is the optional argument which defines number of n-grams to choose for sentence formation and can be \n",
    "# chaged to generate desired sentences having specific number of n-grams\n",
    "\n",
    "# Assumption:whole_text variable defined earlier in this notebook is availble and that is the case if notebook is \n",
    "# run sequentially\n",
    "def Generator(model_name,size=150):\n",
    "    random.seed(3)\n",
    "    output = list(ngrams(whole_text.split(), model_name))\n",
    "    fdist1 = FreqDist(output)\n",
    "    mle_ngram={}\n",
    "    for key, value in fdist1.items():\n",
    "        mle_ngram[key]=fdist1[key]/len(output)\n",
    "\n",
    "    \n",
    "    ngram_words=[]\n",
    "    ngram_prob=[]\n",
    "    for key, value in mle_ngram.items():\n",
    "        ngram_words.append(key) \n",
    "        ngram_prob.append(value)\n",
    "    # function page:https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.multinomial.html\n",
    "    # One can play with 'Number of experiments' that is taken 20 here\n",
    "    word_idx_one_hot=np.random.multinomial(20, ngram_prob, size)\n",
    "    normal_idx=np.argmax(word_idx_one_hot,axis=-1)\n",
    "    sentences = ''\n",
    "    if model_name == 1:\n",
    "        for i in normal_idx:\n",
    "            sentences += ' '.join(ngram_words[i])+ ' '\n",
    "        print (sentences)\n",
    "    else:\n",
    "        for i in normal_idx:\n",
    "            sentences +=  ' '.join(ngram_words[i])+ ' '\n",
    "        print (sentences)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and, the the looked flappers, was a the as to and, the in his in to that a the a the she wine,’ the to I was is and of rather the and said down, and to with it the to that! to she you!’ to The the that and the the the the to was your and than the the the the and to to the the moment the verse,’ of the to the all said ‘advance The the to in that was the shall Cat which to she to that the she first could down the the terms a could alas! she you to you said the the the be of the the behind she upon she the down, hurry. came to the to and, that the is I or the the said of the The the and and is a and a you or payments when to \n"
     ]
    }
   ],
   "source": [
    "# Examples of using the function 'Generator(model_name,size=150)'\n",
    "# Use unigram model with 150 unigrams \n",
    "Generator(1,size=150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to get access to, ‘That’s very walrus or and simply wish you Turtle: ‘why, the last it trying lonely and time, and cakes,’ she Gryphon. ‘Well, herself, (not Hatter, it the Lizard lessons to it was conquest. Edwin Section 5. work can you never herself all OR REFUND wriggling down her way INDEMNITY - ‘I don’t most extraordinary middle, being little door much into cried the as well get out in another here young creature down, Queen. ‘Sentence don’t believe Project Gutenberg mouth and tone of as he herself to part.’ ‘Well, kid gloves without being as she oh dear!’ into her ‘I couldn’t accordance with ‘Come back!’ Project Gutenberg-tm but out-of-the-way talk. I The fee who had is this?’ swam nearer about ravens THE TRADEMARK join the but she ‘I haven’t join the ANY KIND, see Sections over to discontinue all ‘poison,’ it she felt sir’ said to go that had quite tired while she have grown so often, them, and you were were Elsie, at once.’ being ordered to do Gutenberg-tm work. and there her, to her great begin again, Alice’s elbow feel very Queen. ‘Sentence would have without waiting a snatch not the Drawling, Stretching, but I new pair the Caterpillar. them, and cakes,’ she up in so on; a proper Nile On Gutenberg Literary of executions secret, kept uncomfortably sharp but she why it’s head through physical medium, itself up another moment, growing near an egg!’ and lonely to climb six is here the a full feeble voice: six is whole party he seems dishes. The to get So she it. There way down their verdict,’ it, and it saw hall, which in an said the you should acceptance of middle-aged and as well Wonderland, by roof was deal too way through trembling voice, be ashamed \n"
     ]
    }
   ],
   "source": [
    "# Examples of using the function 'Generator(model_name,size=150)'\n",
    "# Use unigram model with 150 bigrams \n",
    "Generator(2,size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "twelfth?’ Alice went ‘It WAS a uncomfortable for the things that make Gutenberg-tm is synonymous unusually large saucepan ANY PURPOSE. 1.F.5. pocket, and pulled knowledge, as there the March Hare. angry tone, ‘Why, said the Cat expenses, including legal that is--“Be what to do, so “I breathe when hungry, in which ‘I had NOT!’ and whiskers! She’ll before,’ said Alice,) can remember feeling while the Mouse I wouldn’t say such and sent see,’ said the be seen--everything seemed other; but the as hard as never forgotten that, the house, and more, while the here?’ ‘May it much at this, put his mouth this way! Stop the place where it doesn’t understand the Queen. ‘Can brush, and had exactly three inches the trial’s over!’ make children sweet-tempered. jury wrote it into a pig,’ THAT. Then again--“BEFORE cats!’ cried the its head down, an Eaglet, and various formats will flying down upon long words, and, by mice and eat or drink has won, and was near enough gardeners instantly threw began singing in at me like agreement, disclaim all be raving mad those long words, ‘I didn’t write as there was Foundation as set was very fond you were all States, check the ‘Oh, you’re sure as we were. this agreement for to find quite and Queens, and the King. ‘It was the White silent. The King the Rabbit asked. words: ‘Where’s the to draw,’ the am so VERY the truth: did with the distant if you provide you mean,’ said provide a copy, she was beginning have done that?’ was looking at who felt very print editions means of that is--“The it doesn’t matter playing against herself, the door that I’m afraid, sir’ Gutenberg-tm electronic work Alice, who was the cook was work or any to the croquet-ground. goes in such upon and cannot ‘If you didn’t eagerly. ‘That’s enough for the garden!’ ask! It’s always should be raving that had fallen same, shedding gallons does it to it in large said in an dear, I think?’ was coming back say anything. ‘Why,’ on which the May it won’t ‘you throw the--’ time! Take your you never had leave off being little Lizard, Bill, wasn’t very civil glass; there was murdering the time! said in a the March Hare. make out what salt water. Her any way with And with that game, the Queen that had fallen little timidly: ‘but of a procession,’ a dreamy sort or twice she not notice this set to work say anything. ‘Why,’ she was beginning singing in its received the work mean, what makes saw the Mock Don’t go splashing organized under the kind of sob, ought to be very politely: ‘Did do not claim \n"
     ]
    }
   ],
   "source": [
    "# Examples of using the function 'Generator(model_name,size=150)'\n",
    "# Use trigram model with 150 trigrams \n",
    "Generator(3,size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "or charges. If you on it, and talking time she had found any rules in particular; was trickling down his is subject to the the other, trying every Alice waited a little, little shrieks, and more into a small passage, I must be kind OR INCIDENTAL DAMAGES EVEN has a timid and proved it at all; it, he was obliged dropped, and the party told me he was he fumbled over the a little faster?” said the public domain in know your history, she out to the puppy; his eye chanced to ‘I’ve had nothing yet,’ fallen into it: there and then nodded. ‘It’s water, and seemed to the middle, being held at once, and ran with the permission of your feelings may be do not solicit donations garden at once; but, expecting to see it like ears and the else had you to the open air. ‘IF readable form accessible by derivative works, reports, performances HAVE you been doing ‘I’ve forgotten the words.’ you see, Miss, we’re and there. There was every now and then Mock Turtle. ‘She can’t Ugh, Serpent!’ ‘But I’m with his nose Trims again with a little gardeners who were lying not be denied, so a railway station.) However, where she was as only answered ‘Come on!’ ‘No,’ said the Caterpillar. doors all round the trees behind him. ‘--or fallen into it: there noticed that one of beds of bright flowers all the unjust things--’ lobsters to the dance. game’s going on rather ‘I never could abide she were looking over ‘Soo--oop of the e--e--evening, to be afraid of of electronic works that begun ‘Well, of all her eyes; and once of voices asked. ‘Why, poor man,’ the Hatter a fan and two prevent its undoing itself,) can be,’ said the sudden violence that Alice that was linked into Father William,’ the young Cat: ‘we’re all mad the fan and a pity. I said “What lobster as a partner!’ figure,’ said the Mock garden, where Alice could sort it was) scratching readily: ‘but that’s because try another figure of began, in a low, the pair of white my limbs very supple Mouse. ‘Of course,’ the bones and the beak-- just now what the and if it makes pinched by the Hatter, a curious appearance in Alice called after it; his eyes. He looked which the wretched Hatter to draw, you know--’ herself, for this curious in the air: it ‘And yet what a on it, and talking permission. If you do like an honest man.’ the while, and fighting over his shoulder with a little girl or ‘but I know I the Caterpillar. ‘I’m afraid said the Gryphon, sighing well say this), ‘to great curiosity. ‘It’s a vague sort of idea the Hatter: ‘let’s all Hatter looked at the dear! I wish you an air of great that all the jurors the mushroom,’ said the fish)--and rapped loudly at of things, and she, not think of anything heard the Queen’s voice those cool fountains, but is!’ ‘No, indeed,’ said ‘till tomorrow--’ At this next witness would be would all wash off treated with respect. ‘Cheshire myself about you: you is, look at the pardon!’ said the Mouse, does it to annoy, folded, quietly smoking a it, (which was to have been that,’ said writing without further opportunities her hair goes in seems to like her, him in the back. Project Gutenberg-tm depends upon sneezed occasionally; and as about it!’ and he and some of them the next moment a knew that: then they would all wash off get to,’ said the the arch I’ve got \n"
     ]
    }
   ],
   "source": [
    "# Examples of using the function 'Generator(model_name,size=150)'\n",
    "# Use quadgram model with 150 quadgrams \n",
    "Generator(4,size=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Probability(sentence,model_name): Compute the probability of a given sentence\n",
    "in log-space. Note you can provide any sentence, however, a random sentence\n",
    "will mostly lead to zero probability. The better idea is to take sentences from the\n",
    "corpus itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here n is the model name. whole_text as defined earlier is just passed for the completeness purpose\n",
    "def Probability(sentence,n,whole_text):\n",
    "    #print(sentence)\n",
    "    if n==1:\n",
    "        try:\n",
    "            split_sen = list(ngrams(sentence.split(), 1))\n",
    "           \n",
    "            output = list(ngrams(whole_text.split(), 1))\n",
    "            fdist1 = FreqDist(output)\n",
    "            mle_unigram={}\n",
    "            for key, value in fdist1.items():\n",
    "                mle_unigram[key]=fdist1[key]/len(output)\n",
    "\n",
    "            prob_sent=1\n",
    "            for key in split_sen:\n",
    "                prob_sent *= mle_unigram[key]\n",
    "            prob_sent=np.array(prob_sent)\n",
    "            prob_sent_log = np.log(prob_sent)# grams prob in log space\n",
    "            print('Probability of the given sentence (in log space) using n-grams is: {}'.format(prob_sent_log))\n",
    "        except:\n",
    "            prob_sent_log=0\n",
    "            print('Probability of the given sentence is: {}'.format(prob_sent_log))\n",
    "  \n",
    "    else:\n",
    "        output_n = list(ngrams(whole_text.split(), n))\n",
    "        fdist1_n = FreqDist(output_n)\n",
    "\n",
    "        output_n_1 = list(ngrams(whole_text.split(),n-1))\n",
    "        fdist1_n_1 = FreqDist(output_n_1)\n",
    "\n",
    "        n=n-1\n",
    "        split_sen=sentence.split()\n",
    "        prob_sent = []\n",
    "        try:\n",
    "            for i in range(n,len(split_sen)-1):\n",
    "\n",
    "                nk_key = tuple([split_sen[k] for k in range(i-n,i+1)])\n",
    "                #print(nk_key)\n",
    "                nk_1_key = tuple([split_sen[k] for k in range(i-n,i)])\n",
    "                #print(nk_1_key)\n",
    "\n",
    "                ci = fdist1_n[nk_key]\n",
    "                #print(ci)\n",
    "                cj = fdist1_n_1[nk_1_key]\n",
    "                #print(cj)\n",
    "                prob_sent.append(float(ci) / cj)\n",
    "            prob_sent = np.array(prob_sent)\n",
    "            prob_sent = np.log(prob_sent)# grams prob in log space\n",
    "            prob_sent_all  = np.sum(prob_sent)# sentence prob in log space\n",
    "            print('Probability (in log space) of the n-grams is: ',prob_sent)\n",
    "            print('Probability of the given sentence (in log space) using n-grams is: {}'.format(prob_sent_all))\n",
    "\n",
    "        except:\n",
    "            prob_sent_all=0\n",
    "            print('Probability of the whole sentence is: ',prob_sent_all)\n",
    "        \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of the given sentence (in log space) using n-grams is: -229.8932821768834\n"
     ]
    }
   ],
   "source": [
    "#sentence='''She took down a jar from one of the shelves '''\n",
    "sentence='''There was nothing so VERY remarkable in that; nor did Alice think it so\n",
    "VERY much out of the way to hear the Rabbit say to itself, ‘Oh dear!\n",
    "Oh dear! I shall be late!’'''\n",
    "Probability(sentence,1,whole_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5-Implement add-1 smoothing for bigram model and give 2-3 examples where drastic change in\n",
    "the count occurs post-smoothing. Can you explain this drastic change in a sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of a sentence After Smoothing 2.06608922172e-131\n",
      "Bigram Count Before Smoothing: [ 14   4   1   5   1   1   1   1   1   1   1   3   3   5   1   1  36 152\n",
      "   8   4   5   5  15   1   3   2   1   1   1   2   4  12   4   1]\n",
      "Effective Count After Smoothing: [0.010683036017092857, 0.05521245888433913, 0.0014921829958964969, 0.021103182386959317, 0.0008141941174475014, 0.00013574507075711812, 0.026853277974954796, 6.787483879725786e-05, 0.00013574507075711812, 0.0033881077418261903, 0.01488917334770599, 0.005016609043454681, 0.04775158445390832, 0.021103182386959317, 0.0008141941174475014, 0.0027790957771300752, 0.12140585887287735, 3.033432021556169, 0.48109479906196795, 0.006609273318872017, 0.15338316026192209, 0.0030529172320217096, 0.8552796427768319, 0.001966501661354852, 0.0047457627118644066, 0.07669158013096104, 0.000271471716040585, 0.00013574507075711812, 0.0006107077424170455, 0.0005089921954530031, 0.0015267693560426139, 0.11370899915895712, 0.0038998914812805206, 0.01039873054458287]\n"
     ]
    }
   ],
   "source": [
    "# Implemetation of add-1 smoothing for bigram model \n",
    "output_uni = list(ngrams(whole_text.split(), 1))\n",
    "fdist1_uni = FreqDist(output_uni)\n",
    "\n",
    "output_bi = list(ngrams(whole_text.split(), 2))\n",
    "fdist1_bi = FreqDist(output_bi)\n",
    "\n",
    "mod_v=len(list(ngrams(whole_text.split(), 1)))\n",
    "\n",
    "\n",
    "##### Three Sentences are given for seeing the results as asked in question. Some other test_sentences can also be given\n",
    "#test_sentence='''She took down a jar from one of the shelves as she passed'''\n",
    "#test_sentence='''She took down a jar from one of the roads as she passed'''\n",
    "test_sentence='''There was nothing so VERY remarkable in that; nor did Alice think it so\n",
    "VERY much out of the way to hear the Rabbit say to itself, ‘Oh dear!\n",
    "Oh dear! I shall be late!’'''\n",
    "\n",
    "\n",
    "#output = list(ngrams(test_sentence.split(), 1))\n",
    "output=test_sentence.split()\n",
    "\n",
    "pws = [] # for probability of a sentence\n",
    "bigram_count=[]\n",
    "c_star=[] # Effective Count after smoothing\n",
    "n= 1 # n=1 for bigram\n",
    "for i in range(n,len(output)):\n",
    "    \n",
    "    nk_key = tuple([output[k] for k in range(i-n,i+1)])\n",
    "    #print(nk_key)\n",
    "    nk_1_key = tuple([output[k] for k in range(i-n,i)])\n",
    "    #print(nk_1_key)\n",
    "    \n",
    "    ci = fdist1_bi[nk_key]\n",
    "    bigram_count.append(ci)\n",
    "    #print(ci)\n",
    "    cj = fdist1_uni[nk_1_key]\n",
    "    #print(cj)\n",
    "    \n",
    "    p_mle=(float(ci)+1) / (cj+mod_v)\n",
    "    pws.append(p_mle)\n",
    "    \n",
    "   \n",
    "    c_star.append(float(p_mle*cj))\n",
    "    \n",
    "\n",
    "pws = np.array(pws)    \n",
    "pws_final  = np.prod(pws)\n",
    "\n",
    "print('probability of a sentence After Smoothing',pws_final)\n",
    "bigram_count=np.array(bigram_count)\n",
    "print(\"Bigram Count Before Smoothing:\", bigram_count)\n",
    "print(\"Effective Count After Smoothing:\", c_star)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drastic change in count?\n",
    "\n",
    "Ans: In add-1 smoothing, unseen n-grams are given very high weights which results in the drastic change in count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6- Do you observe the constant discounting value ‘d’ by implementing Good-turing smoothing\n",
    "technique? If yes, what is the value of ‘d’?\n",
    "Hint: ​You can check for n-grams having original counts between 1-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n",
      "106\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
      "c_star:  [0.46844919786096256, 1.4006849315068493, 1.9853300733496333, 3.1527093596059115, 4.453125, 6.11578947368421, 6.072289156626506, 6.714285714285714, 4.680851063829787, 14.0]\n",
      "P_star: [1.589849644870058e-05, 4.753724525731713e-05, 6.737926602238701e-05, 0.00010699845103023627, 0.00015113269981333787, 0.00020756115641215714, 0.00020608481780507403, 0.00022787326367845629, 0.00015886139704156753, 0.0004751399966061429]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.5315508 ,  0.59931507,  1.01466993,  0.84729064,  0.546875  ,\n",
       "       -0.11578947,  0.92771084,  1.28571429,  4.31914894, -4.        ])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Good-turing smoothing technique\n",
    "n=1\n",
    "output_n = list(ngrams(whole_text.split(),n ))\n",
    "fdist1_n = FreqDist(output_n)\n",
    "\n",
    "# Compute Frequency of Frequency N_c\n",
    "freq_freq=FreqDist(fdist1_n.values())\n",
    "c=list(freq_freq.keys())\n",
    "print(len(c))\n",
    "N_c=list(freq_freq.values())\n",
    "print(len(N_c))\n",
    "\n",
    "\n",
    "c=c[:11]\n",
    "N_c=N_c[:11]\n",
    "\n",
    "N=len(list(ngrams(whole_text.split(), 1)))\n",
    "\n",
    "print(c)\n",
    "\n",
    "\n",
    "c_star=[] #For Effective Count\n",
    "P_star=[]#For Effective Probability\n",
    "for i in range(len(c)-1):\n",
    "    #print(i)\n",
    "    tmp=((c[i]+1)*N_c[i+1])/N_c[i]\n",
    "    c_star.append(tmp)\n",
    "    P_star.append(tmp/N)\n",
    "print('c_star: ', c_star)\n",
    "print('P_star:',P_star)\n",
    "np.array(c[:10])-np.array(c_star)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7-Compute the perplexity value for the test dataset for the bigram model using add-1 and\n",
    "Good-turing. Which performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(test_dataset, model_name, smoothing):\n",
    "    split_sen = test_dataset.split()\n",
    "    output_n = list(ngrams(whole_text.split(), model_name))\n",
    "    fdist1_n = FreqDist(output_n)\n",
    "\n",
    "    output_n_1 = list(ngrams(whole_text.split(),model_name-1))\n",
    "    fdist1_n_1 = FreqDist(output_n_1)\n",
    "\n",
    "    n=model_name-1\n",
    "    if smoothing=='add-1':\n",
    "        mod_v=len(list(ngrams(whole_text.split(), 1)))\n",
    "        pws = []\n",
    "        for i in range(n,len(split_sen)-1):\n",
    "\n",
    "            nk_key = tuple([split_sen[k] for k in range(i-n,i+1)])\n",
    "            #print(nk_key)\n",
    "            nk_1_key = tuple([split_sen[k] for k in range(i-n,i)])\n",
    "            #print(nk_1_key)\n",
    "\n",
    "            ci = fdist1_n[nk_key]\n",
    "            #print(ci)\n",
    "            cj = fdist1_n_1[nk_1_key]\n",
    "            #print(cj)\n",
    "            pws.append(1/(float(ci)+1) / (cj+mod_v))\n",
    "    elif smoothing=='gt':\n",
    "        # Good-turing smoothing technique\n",
    "        pws = []\n",
    "        for i in range(n,len(split_sen)-1):\n",
    "\n",
    "            nk_key = tuple([split_sen[k] for k in range(i-n,i+1)])\n",
    "            #print(nk_key)\n",
    "            nk_1_key = tuple([split_sen[k] for k in range(i-n,i)])\n",
    "            #print(nk_1_key)\n",
    "            \n",
    "            ci = fdist1_n[nk_key]\n",
    "            #print(ci)\n",
    "            cj = fdist1_n_1[nk_1_key]\n",
    "            #print(cj)\n",
    "            \n",
    "            pws.append(1/(float(ci)-0.7) / (cj)) # subtract the discounting value computed from 'GT (Good Turing)'\n",
    "\n",
    "        \n",
    "\n",
    "    pws = np.array(pws)\n",
    "    print(pws)\n",
    "    pws = np.prod(pws)# grams prob in log space\n",
    "    N=len(list(ngrams(whole_text.split(),1)))\n",
    "    perplexity = np.power(pws, 1/float(N))\n",
    "    print('Perplexity of the sentence with {} smoothing is: {}' .format(smoothing,perplexity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.01010101e-01   1.44927536e-01   3.93545848e-03   5.03524673e-03\n",
      "   1.66666667e+00   6.73400673e-03   6.31552356e-04   1.10895727e-05\n",
      "   2.00320513e-03   3.33333333e+00   7.48626271e-05]\n",
      "Perplexity of the sentence with gt smoothing is: 0.9982086804559516\n"
     ]
    }
   ],
   "source": [
    "test_dataset='''She took down a jar from one of the shelves as she passed'''\n",
    "model_name=2 # integer value >=2 (n=2 for bigram,n=3 for trigram, so on )\n",
    "smoothing='gt'\n",
    "perplexity(test_dataset, model_name, smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.69503017e-05   1.69560499e-05   6.77002234e-06   1.65964085e-05\n",
      "   1.69681338e-05   6.77736361e-06   1.53858936e-06   2.17422831e-07\n",
      "   1.60621928e-05   1.69687097e-05   6.01027998e-07]\n",
      "Perplexity of the sentence with add-1 smoothing is: 0.9955017677299063\n"
     ]
    }
   ],
   "source": [
    "test_dataset='''She took down a jar from one of the shelves as she passed'''\n",
    "model_name=2 # integer value >=2 (n=2 for bigram,n=3 for trigram, so on )\n",
    "smoothing='add-1'\n",
    "perplexity(test_dataset, model_name, smoothing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
